# GINR
This is the implementation of Generative implicit neural representation.
We plan to train functa/mnif/mod-inr as backbone representation for image, point cloud, and nerf.
The latents are generated by lnvae.

# Training
```bash
# train functa on shapenet
python train_inr.py --config cfgs/train_functa_shapenet.yml --log_dir logs
# train mnif on shapenet
python train_inr.py --config cfgs/train_mnif_shapenet.yml --log_dir logs
# train loe on shapenet
python train_inr.py --config cfgs/train_loe_shapenet.yml --log_dir logs
```
## Configuration Explanation
 - `cache_latents`: Cache the latents for each epoch, do not start from the random noise.
 - `norm_latents`: Use normalization for latents embedding module. This avoids under training of certain experts. Which should be enabled for mnif and loe.

# TODO
- [x] Train functa on shapenet
- [x] Train mnif on shapenet
- [x] Train loe on shapenet
- [x] Train functa with hierarchical vae on shapenet
- [x] Train mnif with hierarchical vae on shapenet
- [ ] Train loe with hierarchical vae on shapenet
- [ ] Train functa with layer vae
- [ ] Train mnif with layer vae
- [ ] Train loe with layer vae
- [ ] Add mixture-pf-depth module
